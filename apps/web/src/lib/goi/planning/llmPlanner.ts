/**
 * GOI LLM è§„åˆ’å™¨
 *
 * å½“æ²¡æœ‰åŒ¹é…æ¨¡æ¿æ—¶ï¼Œä½¿ç”¨ LLM ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
 */

import { nanoid } from 'nanoid'
import type {
  ResourceType,
  GoiOperation,
  TaskPlan,
  PlanStep,
  PlanGroup,
  PlanCheckpointType,
  ResourceRequirement,
} from '@platform/shared'
import { prisma } from '../../prisma'
import { invokeModel, type ModelConfig } from '../../modelInvoker'

// ============================================
// ç±»å‹å®šä¹‰
// ============================================

/**
 * è§„åˆ’ä¸Šä¸‹æ–‡
 */
export type PlanningContext = {
  /** å½“å‰é¡µé¢è·¯å¾„ */
  currentPage?: string
  /** æœ€è¿‘è®¿é—®çš„èµ„æº */
  recentResources?: Array<{
    type: ResourceType
    id: string
    name: string
  }>
  /** ç”¨æˆ·åå¥½ */
  preferences?: {
    autoConfirmCheckpoints?: boolean
    skipOptionalSteps?: boolean
  }
  /** å¯ç”¨èµ„æºç»Ÿè®¡ */
  availableResources?: {
    prompts: number
    datasets: number
    models: number
    evaluators: number
  }
}

/**
 * LLM è§„åˆ’å™¨é…ç½®
 */
export type LLMPlannerConfig = {
  /** æ¨¡å‹ ID */
  modelId: string
  /** æ¸©åº¦ */
  temperature?: number
  /** æœ€å¤§ token */
  maxTokens?: number
}

/**
 * LLM è§„åˆ’å“åº”
 */
type LLMPlanResponse = {
  summary: string
  steps: Array<{
    order: number
    type: 'navigate' | 'select' | 'create' | 'edit' | 'delete' | 'execute' | 'wait'
    resource: string
    resourceName?: string
    userLabel: string
    hint?: string
    isCheckpoint?: boolean
    checkpointReason?: string
    dependencies?: number[]
    isOptional?: boolean
    group?: string
  }>
  requiredResources: Array<{
    type: string
    name?: string
    isRequired: boolean
  }>
}

// ============================================
// è§„åˆ’æç¤ºè¯
// ============================================

const PLANNING_PROMPT = `ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„ç›®æ ‡æ‹†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ã€‚

## å¹³å°èƒ½åŠ›

è¯¥å¹³å°æ˜¯ä¸€ä¸ª AI æ¨¡å‹æµ‹è¯•å¹³å°ï¼Œæ”¯æŒä»¥ä¸‹æ“ä½œï¼š

### èµ„æºç±»å‹
- prompt: æç¤ºè¯
- dataset: æ•°æ®é›†
- model: AI æ¨¡å‹
- evaluator: è¯„ä¼°å™¨
- task: æµ‹è¯•ä»»åŠ¡
- scheduled_task: å®šæ—¶ä»»åŠ¡
- alert_rule: å‘Šè­¦è§„åˆ™
- notify_channel: é€šçŸ¥æ¸ é“

### æ“ä½œç±»å‹
1. **navigate** - å¯¼èˆªåˆ°é¡µé¢
2. **select** - é€‰æ‹©å·²æœ‰èµ„æº
3. **create** - åˆ›å»ºæ–°èµ„æº
4. **edit** - ç¼–è¾‘èµ„æº
5. **delete** - åˆ é™¤èµ„æº
6. **execute** - æ‰§è¡Œä»»åŠ¡
7. **wait** - ç­‰å¾…ç»“æœ

### é¡µé¢è·¯å¾„
- /prompts - æç¤ºè¯åˆ—è¡¨
- /prompts/new - åˆ›å»ºæç¤ºè¯
- /datasets - æ•°æ®é›†åˆ—è¡¨
- /models - æ¨¡å‹é…ç½®
- /evaluators - è¯„ä¼°å™¨åˆ—è¡¨
- /tasks - ä»»åŠ¡åˆ—è¡¨
- /tasks/new - åˆ›å»ºä»»åŠ¡
- /scheduled - å®šæ—¶ä»»åŠ¡
- /monitor - ç›‘æ§ä¸­å¿ƒ
- /settings - ç³»ç»Ÿè®¾ç½®

## è¾“å‡ºæ ¼å¼

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè®¡åˆ’ï¼š

\`\`\`json
{
  "summary": "ç®€è¦æè¿°è¦åšä»€ä¹ˆï¼ˆä¸€å¥è¯ï¼‰",
  "steps": [
    {
      "order": 1,
      "type": "navigate|select|create|edit|delete|execute|wait",
      "resource": "èµ„æºç±»å‹",
      "resourceName": "ç”¨æˆ·æåˆ°çš„èµ„æºåï¼ˆå¦‚æœ‰ï¼‰",
      "userLabel": "ç”¨æˆ·å¯è¯»çš„æè¿°ï¼ˆä¸­æ–‡ï¼‰",
      "hint": "æ“ä½œæç¤ºï¼ˆå¯é€‰ï¼‰",
      "isCheckpoint": trueæˆ–false,
      "checkpointReason": "éœ€è¦ç¡®è®¤çš„åŸå› ï¼ˆå¦‚æœæ˜¯æ£€æŸ¥ç‚¹ï¼‰",
      "dependencies": [ä¾èµ–çš„æ­¥éª¤orderæ•°ç»„],
      "isOptional": trueæˆ–false,
      "group": "prepare|config|execute|verify"
    }
  ],
  "requiredResources": [
    {
      "type": "èµ„æºç±»å‹",
      "name": "ç”¨æˆ·æåˆ°çš„åç§°",
      "isRequired": trueæˆ–false
    }
  ]
}
\`\`\`

## è§„åˆ’åŸåˆ™

1. **åŸå­åŒ–** - æ¯ä¸ªæ­¥éª¤åº”è¯¥æ˜¯å•ä¸€ã€æ˜ç¡®çš„æ“ä½œ
2. **æœ‰åºæ€§** - æ­¥éª¤é¡ºåºåº”è¯¥åˆç†ï¼Œä¾èµ–å…³ç³»æ¸…æ™°
3. **æ£€æŸ¥ç‚¹** - ä»¥ä¸‹æƒ…å†µè®¾ä¸ºæ£€æŸ¥ç‚¹ï¼ˆisCheckpoint: trueï¼‰ï¼š
   - é€‰æ‹©å…³é”®èµ„æºï¼ˆprompt, dataset, modelï¼‰
   - ä¸å¯é€†æ“ä½œï¼ˆåˆ é™¤ã€æäº¤ï¼‰
   - æ¶‰åŠè´¹ç”¨çš„æ“ä½œï¼ˆè°ƒç”¨ LLM APIï¼‰
   - é¦–æ¬¡æ‰§è¡ŒæŸç±»æ“ä½œ
4. **åˆ†ç»„** - å°†æ­¥éª¤åˆ†é…åˆ°åˆé€‚çš„åˆ†ç»„ï¼š
   - prepare: å‡†å¤‡é˜¶æ®µï¼ˆå¯¼èˆªã€æ‰“å¼€å¼¹çª—ï¼‰
   - config: é…ç½®é˜¶æ®µï¼ˆé€‰æ‹©èµ„æºã€å¡«å†™è¡¨å•ï¼‰
   - execute: æ‰§è¡Œé˜¶æ®µï¼ˆæäº¤ã€è¿è¡Œï¼‰
   - verify: éªŒè¯é˜¶æ®µï¼ˆæ£€æŸ¥ç»“æœï¼‰
5. **å®¹é”™æ€§** - æ ‡è®°å¯é€‰æ­¥éª¤ï¼Œæä¾›è·³è¿‡æ¡ä»¶

## ç”¨æˆ·ç›®æ ‡

{goal}

## ä¸Šä¸‹æ–‡ä¿¡æ¯

{context}

è¯·ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼ˆåªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š`

// ============================================
// æ¨¡å‹é…ç½®è·å–
// ============================================

async function getModelConfig(modelId: string): Promise<ModelConfig | null> {
  const syncedModel = await prisma.syncedModel.findUnique({
    where: { id: modelId },
  })

  if (syncedModel) {
    return {
      id: syncedModel.id,
      modelId: syncedModel.modelId,
      provider: {
        type: 'openai',
        baseUrl: '',
        apiKey: '',
        headers: {},
      },
      config: {},
      pricing: {
        inputPerMillion: syncedModel.inputPrice ? syncedModel.inputPrice * 1000 : undefined,
        outputPerMillion: syncedModel.outputPrice ? syncedModel.outputPrice * 1000 : undefined,
        currency: 'CNY',
      },
      source: 'fastgpt',
    }
  }

  const localModel = await prisma.model.findUnique({
    where: { id: modelId },
    include: { provider: true },
  })

  if (localModel) {
    return {
      id: localModel.id,
      modelId: localModel.modelId,
      provider: {
        type: localModel.provider.type,
        baseUrl: localModel.provider.baseUrl,
        apiKey: localModel.provider.apiKey,
        headers: (localModel.provider.headers as Record<string, string>) || {},
      },
      config: (localModel.config as Record<string, unknown>) || {},
      pricing: localModel.pricing as ModelConfig['pricing'],
      source: 'local',
    }
  }

  return null
}

// ============================================
// LLM è°ƒç”¨
// ============================================

async function callLLM(
  prompt: string,
  config: LLMPlannerConfig
): Promise<string> {
  const modelConfig = await getModelConfig(config.modelId)
  if (!modelConfig) {
    throw new Error(`Model not found: ${config.modelId}`)
  }

  const result = await invokeModel(modelConfig, {
    messages: [
      { role: 'user', content: prompt },
    ],
    temperature: config.temperature ?? 0.3,
    maxTokens: config.maxTokens ?? 4000,
  })

  return result.output
}

// ============================================
// å“åº”è§£æ
// ============================================

/**
 * è§£æ LLM å“åº”
 */
function parseLLMResponse(response: string): LLMPlanResponse {
  // æå– JSON å†…å®¹
  const jsonMatch = response.match(/```json\s*([\s\S]*?)\s*```/) ||
                    response.match(/\{[\s\S]*\}/)

  if (!jsonMatch) {
    throw new Error('Failed to extract JSON from response')
  }

  const jsonStr = jsonMatch[1] || jsonMatch[0]

  try {
    return JSON.parse(jsonStr)
  } catch (error) {
    throw new Error(`Failed to parse JSON: ${error}`)
  }
}

/**
 * æ¨æ–­æ£€æŸ¥ç‚¹ç±»å‹
 */
function inferCheckpointType(step: LLMPlanResponse['steps'][0]): PlanCheckpointType | undefined {
  if (!step.isCheckpoint) return undefined

  if (step.type === 'select' && ['prompt', 'dataset', 'model'].includes(step.resource)) {
    return 'resource_selection'
  }
  if (step.type === 'delete') {
    return 'irreversible_action'
  }
  if (step.type === 'execute') {
    return 'cost_incurring'
  }

  return 'user_defined'
}

/**
 * ä¼°ç®—æ­¥éª¤è€—æ—¶
 */
function estimateStepTime(step: LLMPlanResponse['steps'][0]): number {
  switch (step.type) {
    case 'navigate':
      return 2
    case 'select':
      return 5
    case 'create':
      return 15
    case 'edit':
      return 10
    case 'delete':
      return 3
    case 'execute':
      return 30
    case 'wait':
      return 60
    default:
      return 10
  }
}

/**
 * å°†æ­¥éª¤ç±»å‹è½¬æ¢ä¸º GOI æ“ä½œ
 */
function convertToOperation(step: LLMPlanResponse['steps'][0]): GoiOperation {
  const resourceType = step.resource as ResourceType

  switch (step.type) {
    case 'navigate':
      return {
        type: 'access',
        target: { resourceType },
        action: 'navigate',
      }

    case 'select':
      return {
        type: 'access',
        target: { resourceType },
        action: 'select',
      }

    case 'create':
      return {
        type: 'state',
        target: { resourceType },
        action: 'create',
        expectedState: {},
      }

    case 'edit':
      return {
        type: 'state',
        target: { resourceType },
        action: 'update',
        expectedState: {},
      }

    case 'delete':
      return {
        type: 'state',
        target: { resourceType },
        action: 'delete',
        expectedState: {},
      }

    case 'execute':
    case 'wait':
      return {
        type: 'observation',
        queries: [{
          resourceType,
          fields: ['status', 'progress', 'result'],
        }],
      }

    default:
      return {
        type: 'observation',
        queries: [],
      }
  }
}

// ============================================
// è®¡åˆ’ç”Ÿæˆ
// ============================================

/**
 * åˆ†ç»„é…ç½®
 */
const GROUP_CONFIG: Record<string, { name: string; emoji: string }> = {
  prepare: { name: 'å‡†å¤‡', emoji: 'ğŸ“‹' },
  config: { name: 'é…ç½®', emoji: 'âš™ï¸' },
  execute: { name: 'æ‰§è¡Œ', emoji: 'ğŸš€' },
  verify: { name: 'éªŒè¯', emoji: 'âœ…' },
}

/**
 * ç”Ÿæˆåˆ†ç»„
 */
function generateGroups(steps: PlanStep[]): PlanGroup[] {
  const groupMap = new Map<string, string[]>()

  for (const step of steps) {
    const groupId = step.group || 'execute'
    if (!groupMap.has(groupId)) {
      groupMap.set(groupId, [])
    }
    groupMap.get(groupId)!.push(step.id)
  }

  return Array.from(groupMap.entries()).map(([groupId, stepIds]) => {
    const config = GROUP_CONFIG[groupId] || { name: groupId, emoji: 'ğŸ“Œ' }
    return {
      id: groupId,
      name: config.name,
      emoji: config.emoji,
      stepIds,
      collapsed: false,
    }
  })
}

/**
 * å°† LLM å“åº”è½¬æ¢ä¸º TaskPlan
 */
function convertLLMResponseToPlan(data: LLMPlanResponse, goal: string): TaskPlan {
  const planId = nanoid()

  const steps: PlanStep[] = data.steps.map((step, index) => ({
    id: `step-${step.order}`,
    order: step.order,
    operation: convertToOperation(step),
    userLabel: step.userLabel,
    technicalLabel: `${step.type}:${step.resource}`,
    hint: step.hint,
    dependencies: step.dependencies?.map(d => `step-${d}`) || (index > 0 ? [`step-${index}`] : []),
    isCheckpoint: step.isCheckpoint || false,
    checkpointType: inferCheckpointType(step),
    checkpointReason: step.checkpointReason,
    status: 'pending',
    estimatedSeconds: estimateStepTime(step),
    isOptional: step.isOptional || false,
    group: step.group || 'execute',
  }))

  const groups = generateGroups(steps)

  const requiredResources: ResourceRequirement[] = data.requiredResources.map(r => ({
    type: r.type as ResourceType,
    name: r.name,
    isRequired: r.isRequired,
  }))

  return {
    id: planId,
    goal,
    summary: data.summary,
    steps,
    groups,
    requiredResources,
    checkpointStepIds: steps.filter(s => s.isCheckpoint).map(s => s.id),
    estimatedTotalSeconds: steps.reduce((sum, s) => sum + s.estimatedSeconds, 0),
    planSource: 'llm',
    createdAt: new Date(),
    updatedAt: new Date(),
  }
}

// ============================================
// å¯¼å‡ºå‡½æ•°
// ============================================

/**
 * ä½¿ç”¨ LLM ç”Ÿæˆè®¡åˆ’
 */
export async function generatePlanWithLLM(
  goal: string,
  context: PlanningContext,
  config: LLMPlannerConfig
): Promise<TaskPlan> {
  const prompt = PLANNING_PROMPT
    .replace('{goal}', goal)
    .replace('{context}', JSON.stringify(context, null, 2))

  const response = await callLLM(prompt, config)
  const planData = parseLLMResponse(response)

  return convertLLMResponseToPlan(planData, goal)
}

/**
 * éªŒè¯ LLM ç”Ÿæˆçš„è®¡åˆ’
 */
export function validateLLMPlan(plan: TaskPlan): {
  valid: boolean
  errors: string[]
  warnings: string[]
} {
  const errors: string[] = []
  const warnings: string[] = []

  // æ£€æŸ¥æ˜¯å¦æœ‰æ­¥éª¤
  if (plan.steps.length === 0) {
    errors.push('è®¡åˆ’æ²¡æœ‰ä»»ä½•æ­¥éª¤')
  }

  // æ£€æŸ¥ä¾èµ–å…³ç³»
  const stepIds = new Set(plan.steps.map(s => s.id))
  for (const step of plan.steps) {
    for (const dep of step.dependencies) {
      if (!stepIds.has(dep)) {
        errors.push(`æ­¥éª¤ ${step.id} ä¾èµ–ä¸å­˜åœ¨çš„æ­¥éª¤ ${dep}`)
      }
    }
  }

  // æ£€æŸ¥å¾ªç¯ä¾èµ–
  const visited = new Set<string>()
  const visiting = new Set<string>()

  const hasCycle = (stepId: string): boolean => {
    if (visiting.has(stepId)) return true
    if (visited.has(stepId)) return false

    visiting.add(stepId)
    const step = plan.steps.find(s => s.id === stepId)
    if (step) {
      for (const dep of step.dependencies) {
        if (hasCycle(dep)) return true
      }
    }
    visiting.delete(stepId)
    visited.add(stepId)
    return false
  }

  for (const step of plan.steps) {
    if (hasCycle(step.id)) {
      errors.push('è®¡åˆ’å­˜åœ¨å¾ªç¯ä¾èµ–')
      break
    }
  }

  // æ£€æŸ¥æ£€æŸ¥ç‚¹
  if (plan.checkpointStepIds.length === 0) {
    warnings.push('è®¡åˆ’æ²¡æœ‰è®¾ç½®ä»»ä½•æ£€æŸ¥ç‚¹')
  }

  // æ£€æŸ¥èµ„æºéœ€æ±‚
  const requiredResources = plan.requiredResources.filter(r => r.isRequired)
  const unresolvedResources = requiredResources.filter(r => !r.resolved)
  if (unresolvedResources.length > 0) {
    warnings.push(`æœ‰ ${unresolvedResources.length} ä¸ªå¿…éœ€èµ„æºæœªè§£æ`)
  }

  return {
    valid: errors.length === 0,
    errors,
    warnings,
  }
}
